# üî• Teacher Training - Gradient Explosion Emergency Fix

## ‚ùå What You Saw (NOT Normal)

```
WARNING - Large gradient norm: inf at batch 0
WARNING - Large gradient norm: inf at batch 1
WARNING - Large gradient norm: inf at batch 2
...
loss=104.3560, grad_norm=inf
loss=94.4273, grad_norm=inf
```

**This is GRADIENT EXPLOSION** - the model is unstable and will crash soon.

## üîß Emergency Fixes Applied

### 1. Much Lower Learning Rate
```diff
- base_lr = 1e-4  # Was still too high
+ base_lr = 1e-5  # 10x lower for stability
```

### 2. More Aggressive Gradient Clipping
```diff
- max_norm = 1.0  # Still too permissive
+ max_norm = 0.5  # Much stricter clipping
```

### 3. Skip Batches with Infinite Gradients
```python
if torch.isinf(grad_norm) or grad_norm > 10.0:
    logger.warning(f"Infinite gradient, skipping batch")
    continue  # Don't update weights with bad gradients
```

### 4. Smaller Initial Weights
```diff
- nn.init.normal_(m.weight, 0, 0.01)  # Too large
+ nn.init.normal_(m.weight, 0, 0.001)  # 10x smaller

# For Conv layers
m.weight.data *= 0.5  # Scale down by 50%
```

## üöÄ What to Do Now

### Step 1: Stop Current Training
Press `Ctrl+C` in your terminal

### Step 2: Run with Ultra-Stable Config

**Windows**:
```cmd
train_teacher_stable.bat
```

**Linux/Mac**:
```bash
bash train_teacher_stable.sh
```

### Step 3: Watch for Healthy Signs

**Good** ‚úÖ:
```
Epoch 1: loss=15.234, grad_norm=0.48
Epoch 1: loss=14.567, grad_norm=0.45
Epoch 1: loss=13.891, grad_norm=0.43
```
- Loss decreasing
- Gradient norms < 1.0
- NO "inf" warnings

**Bad** ‚ùå:
```
WARNING - Infinite gradient norm at batch X
loss=95.XXX, grad_norm=inf
```
- Still seeing inf
- Need even more aggressive fixes

## üìä What Changed

| Parameter | Before | After Emergency Fix |
|-----------|--------|-------------------|
| **Learning Rate** | 1e-4 | 1e-5 (10x lower) |
| **Grad Clip** | 1.0 | 0.5 (2x stricter) |
| **Skip Inf Grads** | No | Yes ‚úÖ |
| **Weight Init** | 0.01 | 0.001 (10x smaller) |
| **Conv Init Scale** | 1.0 | 0.5 (50% smaller) |
| **Beta2** | 0.999 | 0.98 (more stable) |

## üéØ Expected Behavior Now

### Epoch 1 (First 10 batches)

**Before (Bad)**:
```
Batch 0: loss=104.36, grad_norm=inf ‚ùå
Batch 1: loss=94.43, grad_norm=inf ‚ùå
Batch 2: loss=94.38, grad_norm=inf ‚ùå
```

**After (Good)**:
```
Batch 0: loss=18.45, grad_norm=0.48 ‚úÖ
Batch 1: loss=17.23, grad_norm=0.45 ‚úÖ
Batch 2: loss=16.89, grad_norm=0.43 ‚úÖ
```

### Full Epoch Progression

| Epoch | Loss | WER | Grad Norm | Status |
|-------|------|-----|-----------|--------|
| 1 | 12-15 | 95-98% | 0.3-0.8 | Stable start ‚úÖ |
| 5 | 10-12 | 90-95% | 0.3-0.7 | Learning ‚úÖ |
| 10 | 8-10 | 80-90% | 0.2-0.6 | Improving ‚úÖ |
| 20 | 6-8 | 60-75% | 0.2-0.5 | Good progress ‚úÖ |
| 50 | 3-5 | 25-35% | 0.1-0.4 | Target range ‚úÖ |

**Warning**: Training will be SLOWER due to very low LR, but it will be STABLE.

## üîç Root Cause Analysis

### Why Baseline Works but Teacher Doesn't

| Factor | Baseline | Teacher |
|--------|----------|---------|
| Model depth | 20 layers | 50+ layers |
| Gradient path | Short | Very long |
| Parameters | 15.7M | 40-50M |
| Attention | None | MultiheadAttention |
| Batch size | 4 (stable BN) | 2 (unstable BN) |
| **Stability** | ‚úÖ Inherently stable | ‚ùå Needs careful tuning |

### The Attention Problem

```python
# MultiheadAttention does this internally:
attention_weights = softmax(Q @ K.T / sqrt(d_k))
output = attention_weights @ V

# If Q or K have large values:
# - Q @ K.T ‚Üí huge values
# - softmax ‚Üí extreme (0 or 1)
# - Gradients explode backward
```

**Our fix**: Clamp attention outputs to [-10, 10]

### The Depth Problem

```
Input ‚Üí 50+ layers ‚Üí Output

Gradients flow backward:
Output grad ‚Üí Layer 50 ‚Üí Layer 49 ‚Üí ... ‚Üí Layer 1

At each layer: grad_out * local_grad = grad_in

If local_grad > 1.0 at each layer:
  grad_1 = 1.5^50 = 637,621,500,000 (explosion!)

Our fix: Clip to max_norm=0.5 (keep gradients small)
```

## üõ†Ô∏è Alternative: Disable Mixed Precision

If still unstable, try FP32 only (no autocast):

<function_calls>
<invoke name="read_file">
<parameter name="target_file">src/training/train_teacher.py
