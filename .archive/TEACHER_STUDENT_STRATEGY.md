# Teacher-Student Knowledge Distillation Strategy

## Overview

According to the research proposal, we use a **two-phase training approach**:

1. **Phase I**: Train baseline MobileNetV3 student (Target: 40% WER)
2. **Phase II**: Knowledge distillation from teacher (Target: <25% WER)

## Training Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Teacher Model   â”‚ (Large, accurate but slow)
â”‚  I3D/SlowFast    â”‚
â”‚  ~100-200M paramsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Knowledge Distillation
         â”‚ (Soft targets @ T=3.0)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Student Model   â”‚ (Small, efficient, deployable)
â”‚  MobileNetV3     â”‚
â”‚  15.7M params    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Teacher Model Options

### Option 1: I3D (Inflated 3D ConvNet) âœ… Recommended
- **Pre-trained on**: Kinetics-400/600 action recognition
- **Parameters**: ~25M (I3D) to 35M (I3D with non-local blocks)
- **Why good**: Designed for video understanding, captures temporal dynamics
- **Adaptation**: Fine-tune on sign language dataset first

### Option 2: SlowFast Networks
- **Pre-trained on**: Kinetics, AVA
- **Parameters**: 34M (SlowFast-R50) to 60M (SlowFast-R101)
- **Why good**: Dual pathway for capturing both slow and fast motions
- **Adaptation**: More complex, may be overkill for our features

### Option 3: Custom Large Model (Fallback)
- **Architecture**: Larger BiLSTM or Transformer
- **Parameters**: 50-100M
- **Why**: If pre-trained models don't work with our features
- **Training**: From scratch on sign language data

## Implementation Plan

### Step 1: Prepare Teacher Model

Since we're using **MediaPipe features** (not raw video), we need to adapt video models:

```python
# Approach A: Feature-based I3D (Recommended)
# Adapt I3D to work with MediaPipe features instead of RGB frames
class FeatureI3D(nn.Module):
    def __init__(self, input_dim=6516, hidden_dim=512):
        # 1D convolutions instead of 3D
        # Process temporal features directly
```

### Step 2: Teacher Training Pipeline

1. **Download pre-trained weights** (if using I3D/SlowFast)
2. **Adapt architecture** for MediaPipe features
3. **Fine-tune on sign language**:
   - Start with low learning rate (1e-5)
   - Train until convergence (~20-30% WER)
   - This becomes our teacher

### Step 3: Knowledge Distillation

```python
# Loss function (from proposal)
Loss = 0.7 * L_soft + 0.3 * L_hard

Where:
- L_soft: KL divergence between teacher and student outputs (T=3.0)
- L_hard: CTC loss on ground truth labels
- Temperature T=3.0: Softens probability distributions
```

## Current Training Strategy

### What We're Training Now

We have **THREE** options for proceeding:

#### Option A: Train MobileNetV3 Baseline First (Quick Start) âš¡
```bash
# Train student directly without teacher
python src/training/train.py --epochs 100
# Expected: ~40% WER (Phase I target)
# Time: 1-2 days
```
**Pros**: Can start immediately, establish baseline
**Cons**: Won't achieve <25% WER without distillation

#### Option B: Implement Teacher First (Best Results) ðŸŽ¯
```bash
# 1. Implement I3D teacher
# 2. Train teacher to ~20-30% WER
# 3. Distill to student
# Expected: <25% WER (Phase II target)
# Time: 3-5 days
```
**Pros**: Best final performance
**Cons**: More implementation work upfront

#### Option C: Progressive Training (Balanced) âš–ï¸
```bash
# 1. Train MobileNetV3 baseline (1-2 days)
# 2. While training, implement teacher
# 3. Use baseline as initialization for distillation
# Expected: <25% WER eventually
# Time: Parallel work
```
**Pros**: See results quickly, optimize later
**Cons**: May need to retrain student

## Recommended Approach

### For Research Proposal Compliance: **Option B**

The proposal specifically mentions knowledge distillation as a key component. We should:

1. **Implement I3D teacher** adapted for MediaPipe features
2. **Train teacher** to achieve good performance (~20-30% WER)
3. **Distill to MobileNetV3** using soft targets
4. **Achieve <25% WER** as specified in objectives

### Implementation Priority:

1. âœ… MobileNetV3 student (DONE)
2. â³ I3D teacher model (NEXT)
3. â³ Knowledge distillation training
4. â³ Final evaluation

## Why Knowledge Distillation?

Your research proposal emphasizes this because:

1. **Better Performance**: Teacher provides richer training signal than labels alone
2. **Regularization**: Soft targets prevent overfitting
3. **Dark Knowledge**: Teacher encodes relationships between classes
4. **Proven Method**: Standard approach in model compression

## Key Files Needed

```
src/models/
â”œâ”€â”€ mobilenet_v3.py      âœ… Student model (DONE)
â”œâ”€â”€ i3d_teacher.py       âŒ Teacher model (TODO)
â””â”€â”€ distillation.py      âŒ Distillation utilities (TODO)

src/training/
â”œâ”€â”€ train.py             âœ… Basic training (DONE)
â”œâ”€â”€ train_teacher.py     âŒ Teacher training (TODO)
â””â”€â”€ train_distillation.py âŒ Knowledge distillation (TODO)
```

## Decision Required

**Question**: Should we:

1. **Start training MobileNetV3 baseline** now (get results in 1-2 days)?
2. **Implement I3D teacher first** (better final results, 3-5 days)?
3. **Do both in parallel** (you train baseline while I implement teacher)?

The research proposal suggests Option 2 for best results, but Option 3 might be more practical.

## Next Steps

If you want to follow the proposal exactly:
1. Let me implement the I3D teacher model
2. Train it on your sign language data
3. Use it to distill knowledge to MobileNetV3
4. Achieve the <25% WER target

This is the path to the best performance as outlined in your research objectives.