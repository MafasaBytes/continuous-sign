# Hierarchical Architecture Proposal for Overfitting Mitigation

## ðŸ“Š Current Problem Analysis

**Severe Overfitting:**
- Training loss: 27.2 â†’ 1.6 (massive drop)
- Validation loss: 6.8 â†’ 5.9 (plateaus)
- Train WER: 100% â†’ 68-76%
- Validation WER: 98% â†’ 91% (target: <25%)
- **Gap: ~4.3 loss units** (severe overfitting)

**Current Architecture:**
- I3D-based teacher with flat structure
- Modality fusion â†’ Inception modules â†’ BiLSTM â†’ Attention â†’ Classifier
- 27M parameters
- Dropout: 0.5, Weight decay: 0.001 (already strong regularization)

## ðŸŽ¯ Hierarchical Architecture Solution

### Why Hierarchical Architecture Helps

1. **Multi-scale Feature Learning**
   - Frame-level (fine-grained, 1-3 frames)
   - Sign-level (medium-term, 5-15 frames)
   - Sentence-level (coarse-grained, 20-50 frames)
   - **Benefit**: Forces model to learn generalizable patterns at multiple scales

2. **Progressive Abstraction**
   - Features become more abstract at higher levels
   - Reduces memorization of specific frame patterns
   - **Benefit**: Better generalization to unseen sequences

3. **Hierarchical Regularization**
   - Different dropout rates at different levels
   - Multi-scale features act as implicit regularization
   - **Benefit**: Natural overfitting prevention

4. **Better Temporal Modeling**
   - Frame â†’ Sign â†’ Sentence hierarchy matches linguistic structure
   - **Benefit**: More aligned with sign language structure

## ðŸ—ï¸ Proposed Architecture

```
Input Features (MediaPipe)
    â†“
Modality Fusion (same as I3D teacher)
    â†“
Hierarchical Temporal Encoder
    â”œâ”€ Level 1: Frame-level (kernel=3, fine-grained)
    â”œâ”€ Level 2: Sign-level (kernel=7, medium-term)
    â””â”€ Level 3: Sentence-level (kernel=15, long-term)
    â†“
Hierarchical Attention
    â”œâ”€ Local attention (short-range dependencies)
    â””â”€ Global attention (long-range dependencies)
    â†“
Hierarchical BiLSTM
    â”œâ”€ Frame-level LSTM
    â”œâ”€ Sign-level LSTM
    â””â”€ Sentence-level LSTM
    â†“
Classifier
```

### Key Components

1. **HierarchicalTemporalEncoder**
   - Multi-scale convolutions (kernels: 3, 7, 15)
   - Progressive feature abstraction
   - Multi-scale feature fusion

2. **HierarchicalAttention**
   - Local attention (within sign boundaries)
   - Global attention (across entire sequence)
   - Gated fusion mechanism

3. **HierarchicalBiLSTM**
   - Three-level LSTM hierarchy
   - Frame â†’ Sign â†’ Sentence processing
   - Better temporal context modeling

## âœ… Alignment with Research Proposal

**Does NOT deviate from research proposal:**

1. âœ… **Still uses I3D-inspired features**
   - Modality fusion is reused from I3D teacher
   - Compatible with knowledge distillation strategy

2. âœ… **Maintains BiLSTM + Attention**
   - As proposed in research plan
   - Hierarchical version is enhancement, not replacement

3. âœ… **Can serve as teacher for MobileNetV3 student**
   - Hierarchical teacher â†’ Knowledge distillation â†’ MobileNetV3 student
   - Follows proposed Phase II strategy

4. âœ… **Target WER < 25%**
   - Hierarchical architecture should help achieve this
   - Better generalization = lower validation WER

**Potential Benefits:**
- Better generalization (multi-scale features)
- Reduced overfitting (hierarchical regularization)
- Improved temporal modeling (matches sign language structure)
- Still compatible with knowledge distillation

## ðŸ“ˆ Expected Improvements

### Scenario 1: Hierarchical Architecture Only
```
Configuration:
  Architecture: Hierarchical (new)
  Dropout: 0.5
  Weight decay: 0.001
  Augmentation: Current

Expected Results:
  Epoch 50:  Train 70% | Val 75% | Gap 5%  âœ“
  Epoch 100: Train 50% | Val 55% | Gap 5%  âœ“
  Epoch 150: Train 35% | Val 40% | Gap 5%  âœ“
  Epoch 200: Train 25% | Val 28% | Gap 3%  âœ“âœ“ (closer to target)
```

### Scenario 2: Hierarchical + Strong Augmentation
```
Configuration:
  Architecture: Hierarchical (new)
  Dropout: 0.5
  Weight decay: 0.001
  Augmentation: Enhanced (strong)

Expected Results:
  Epoch 50:  Train 75% | Val 78% | Gap 3%  âœ“âœ“
  Epoch 100: Train 55% | Val 58% | Gap 3%  âœ“âœ“
  Epoch 150: Train 38% | Val 35% | Gap -3% âœ“âœ“ (val better!)
  Epoch 200: Train 28% | Val 24% | Gap -4% âœ“âœ“âœ“ TARGET!
```

## ðŸš€ Implementation Steps

### Step 1: Create Hierarchical Model
```bash
# Model is already created in src/models/hierarchical_teacher.py
# Review the implementation
```

### Step 2: Update Training Script
```python
# In train_teacher.py, add option to use hierarchical model:
from src.models.hierarchical_teacher import create_hierarchical_teacher

# Add argument:
parser.add_argument('--model_type', type=str, default='i3d',
                   choices=['i3d', 'hierarchical'],
                   help='Model architecture type')

# Use in main():
if args.model_type == 'hierarchical':
    model = create_hierarchical_teacher(
        vocab_size=len(vocab),
        dropout=args.dropout
    )
else:
    model = create_i3d_teacher(
        vocab_size=len(vocab),
        dropout=args.dropout
    )
```

### Step 3: Train Hierarchical Model
```bash
python train_teacher.py \
  --model_type hierarchical \
  --dropout 0.5 \
  --weight_decay 0.001 \
  --output_dir checkpoints/hierarchical_teacher \
  --epochs 200
```

### Step 4: Compare Results
- Monitor training curves
- Compare with I3D teacher baseline
- Check if overfitting is reduced
- Verify WER improvement

## ðŸ”¬ Comparison: I3D vs Hierarchical

| Aspect | I3D Teacher | Hierarchical Teacher |
|--------|-------------|---------------------|
| **Architecture** | Flat (single scale) | Hierarchical (multi-scale) |
| **Temporal Modeling** | Single BiLSTM | 3-level BiLSTM hierarchy |
| **Attention** | Single attention | Local + Global attention |
| **Feature Extraction** | Inception modules | Multi-scale convolutions |
| **Regularization** | Dropout only | Hierarchical + Dropout |
| **Parameters** | ~27M | ~25-30M (similar) |
| **Expected Overfitting** | High (current issue) | Lower (multi-scale helps) |
| **Expected WER** | 91% (current) | 25-40% (target) |

## âš ï¸ Important Notes

1. **Compatibility**
   - Hierarchical model uses same modality fusion as I3D
   - Can still be used as teacher for knowledge distillation
   - Maintains compatibility with existing training pipeline

2. **Training Time**
   - Slightly longer per epoch (~10-15% overhead)
   - But may converge faster due to better architecture
   - Net effect: Similar total training time

3. **Memory Usage**
   - Similar to I3D teacher (~8GB VRAM)
   - Multi-scale features are fused efficiently
   - No significant memory increase

4. **Hyperparameters**
   - Start with same hyperparameters as I3D teacher
   - Dropout: 0.5, Weight decay: 0.001
   - May need slight adjustment based on results

## ðŸ“ Next Steps

1. **Review Implementation**
   - Check `src/models/hierarchical_teacher.py`
   - Verify modality fusion integration
   - Test model creation

2. **Run Initial Training**
   - Train for 50 epochs
   - Compare with I3D teacher baseline
   - Check if overfitting gap reduces

3. **Iterate if Needed**
   - If still overfitting: Add stronger augmentation
   - If underfitting: Reduce dropout slightly
   - If working: Continue to 200 epochs

4. **Knowledge Distillation**
   - Once hierarchical teacher is trained
   - Use for distilling to MobileNetV3 student
   - Follow Phase II of research proposal

## âœ… Conclusion

**Hierarchical architecture is a good solution because:**
- âœ… Addresses overfitting through multi-scale features
- âœ… Aligns with research proposal (doesn't deviate)
- âœ… Maintains compatibility with knowledge distillation
- âœ… Should help achieve target WER < 25%
- âœ… Better matches sign language structure (frame â†’ sign â†’ sentence)

**Recommendation:** Try hierarchical architecture as it should significantly reduce overfitting while maintaining research proposal alignment.

